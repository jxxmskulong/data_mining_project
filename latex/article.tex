%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (10/6/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	12pt, % Main document font size
	a4paper, % Paper type, use 'letterpaper' for US Letter paper
	oneside, % One page layout (no page indentation)
	%twoside, % Two page layout (page indentation for binding and different headers)
	headinclude,footinclude, % Extra spacing for the header and footer
	BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Generazione delle categorie di wikipedia attraverso il clustering}} % The article title

\author{\spacedlowsmallcaps{Cazzaro Dalla Cia Lovisotto Vianello}}

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block

%\newpage

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduzione}

Per il progetto del corso di Data Mining abbiamo deciso di svolgere la traccia proposta B, che proponeva di indagare fino a che punto un clustering sulle pagine di Wikipedia è consistente con le categorie associate alle pagine stesse.
Per fare ciò avevamo a disposizione due dataset, di differenti grandezze, formati da articoli di Wikipedia in lingua inglese.

A partire da tali premesse ci siamo prefissati i seguenti quesiti:

\begin{itemize}
	\item quanto sono clusterizzabili gli articoli del dataset?
	\item le categorie associate a questi articoli sono sensate?
	\item quali sono dei metodi validi per rappresentare opportunamente i nostri dati in input?
	\item come variano i cluster utilizzando tecniche diverse?
	\item c'è un numero ottimale di cluster per il dataset? Quanto vale?
	\item qual è il rapporto tra il cluster ottenuto e le categorie?
\end{itemize}

Tali quesiti troveranno risposta nel seguito della trattazione, che è strutturata nel modo che segue.
La sezione 2 illustra l'analisi preliminare effettuata sugli articoli e sulle categorie.
Successivamente il capitolo 3 descrive le tecniche utilizzate per rappresentare il dataset.
Il capitolo 4 tratta poi degli algoritmi di clustering applicati al dataset e delle tecniche di valutazione impiegate.
Di seguito la sezione 5 riporta e discute i risultati ottenuti nel capitolo precedente, offrendo anche un confronto tra i metodi utilizzati.
Infine la sezione 6 riassume il lavoro svolto, ciò che abbiamo ottenuto e propone alcuni spunti di ricerca per futuri sviluppi.


\section{Dataset e Analisi Preliminare}
	Ci sono stati messi a disposizione due dataset, di differenti grandezze, per effettuare questa analisi.
	Nello specifico abbiamo utilizzato quello minore, il quale è un \emph{dump} composto da un JSON contenente centomila articoli di Wikipedia in versione inglese.
	Per ogni articolo abbiamo a disposizione il titolo, il testo, l'id e le categorie dell'articolo.
	In particolare le categorie sono assegnate a discrezione degli autori e dei successivi revisori di ogni voce, in quanto Wikipedia non prevede una struttura o dei vincoli particolari per l'assegnazione di queste categorie.

	La nostra analisi mira a valutare la relazione semantica tra gli articoli e le loro categorie, quindi prima di effettuare l'analisi si è deciso di eliminare tutti gli articoli che non risultano essere associati a nessuna categoria. Queste pagine sono dette \emph{di disambiguazione} e quindi non sono utili per i nostri scopi.

	Per quanto concerne le categorie, una prima ispezione manuale rivela che esse sono alquanto arbitrarie e spesso così specifiche da essere associate ad un articolo soltanto.
	Esempio di ciò sono le categorie "Roads on the National Register of Historic Places in Illinois", "United Nations Security Council resolutions concerning Sudan" e "Singaporean people of Yemeni descent".
	C'è inoltre da sottolineare come ad ogni articolo siano spesso associate più di una categoria, con una media di 1.99 categorie per ogni voce del dataset.

    Un'analisi più approfondita sulla distribuzione delle categorie è riportata in figura \ref{fig:categories}.
    \begin{figure}[!htb]
			\centering
			\includegraphics[scale=.5]{Figures/categories.png}
			\caption{Distribuzione delle categorie. Nota: per rendere il grafico leggibile il picco finale raggruppa il numero di categorie con cento o più articoli.}
			\label{fig:categories}
		\end{figure}

	L'analisi effettuata rivela infatti che, sul totale di 198609 distinte categorie presenti nel dataset, circa due terzi (precisamente 134524) di queste sono uniche, ovvero sono associate ad un solo articolo.
	Dato che queste non portano alcun contenuto informativo utile alla clusterizzazione, abbiamo provveduto ad escludere tali categorie dalle successive analisi.

	Nondimeno è anche presente un numero ristretto di 158 categorie associate a 100 o più articoli, le quali sono composte per lo più da categorie del genere "1900 deaths" e "1900 births" che si ripetono variando solamente l'anno.
	In tabella \ref{table:toptencategories} sono riportate a titolo di esempio le categorie con più articoli associati.


	\begin{table}[]
	\hspace*{-2cm}\begin{tabular}{l|l||l|l}
	Living people & 14994 & Association football midfielders & 611 \\
	Year of birth missing (living people) & 1172 & Association football defenders & 502 \\
	Place of birth missing (living people) & 751 & Association football forwards & 409 \\
	American films & 741 & Year of birth unknown & 404 \\
	English-language films & 691 & English Football League players & 398 \\
	\end{tabular}
	\caption{Le categorie con maggior numero di articoli associati}
	\label{table:toptencategories}
	\end{table}


\section{Rappresentazione del Dataset}
	Prima di procedere con qualsiasi operazione sull'intero corpus si è deciso di preprocessare il data set eliminando le cosiddette \emph{stop words} presenti nel testo e lemmatizzando le parole rimaste.

	Per l'eliminazione delle \emph{stop words} ci siamo basati su una lista di parole fornita dal sito \url{http://www.ranks.nl/stopwords}.
	Questi termini vengono filtrati dal corpus in quanto portano un contenuto informativo sull'argomento dell'articolo pressoch\'{e} nullo.

	La lemmatizzazione invece è stata eseguita utilizzando il $Lemmatizer$ di Spark al fine di raggruppare assieme le variazioni semantiche delle parole.

	\subsection{VETTORIALIZZIONE DEGLI ARTICOLI}
		Per interagire con i più comuni algoritmi di clustering, ad esempio K-means, si è reso necessario trasformare gli articoli di Wikipedia in vettori.
		Per fare ciò abbiamo adottato una tecnica nota in letteratura con il nome \emph{Word2Vec}. Tale strumento, ideato da Tomas Mikolov, non è altro che una rete neurale a due strati il cui scopo è quello di trasformare parole del linguaggio naturale in vettori. Nello spazio vettoriale generato, le parole semanticamente più simili saranno più vicine, mentre parole che esprimono concetti differenti risulteranno distanti.

		Tale funzionalità è già implementata nella suite software di Spark, ma per sfruttarla al meglio è necessario settare alcuni parametri. Tra questi uno dei più importanti è la dimensione del vettore in uscita.
		In letteratura si è valutato che una dimensionalità nel ordine dei 100/300\cite{w2vdim} risulta un buon compromesso tra prestazioni e capacità di descrivere il dataset.

		Dopo aver allenato il modello \emph{Word2Vec} sul nostro corpus di testi, trasformando così le singole parole in vettori, ad ogni articolo è stata associata la media vettoriale di tutte le parole presenti nel suo testo. Il \emph{Basic Linear Algebra Subprograms} di Spark ci ha permesso di eseguire questa operazione molto rapidamente.

	\subsection{BAG OF WORDS} \label{sec:bag_of_words}

	L'algoritmo Bag of Words\cite{bagofwords} effettua la conversione degli articoli in vettori considerando le occorrenze dei termini in essi.
	In particolare viene considerata come metrica la \textit{Term frequency-inverse document frequency} (Tf-Idf).

	Si definisce la Tf-Idf relativa all'$i$-esimo termine nel $j$-esimo articolo come:
	$$ w_{i,j}=tf_{i,j}\times\log \left(\frac{N}{df_{i}} \right) $$
	dove $tf_{i,j}$ rappresenta il numero di occorrenze di $i$ in $j$, $df_{i}$ è il numero di documenti contenenti il termine $i$, ed $N$ è il numero totale di documenti.

	Abbiamo scelto di limitare l'analisi alle 3000 parole pi\`{u} frequenti nel dataset (escludendo le stopwords).

	Pertanto il modello prodotto associa ad ogni documento un vettore di dimensione 3000, contenente gli indici TfIdf associati ad ogni articolo per tutte le parole selezionate.

\section{Clustering}

	\subsection{Tecniche di Clustering}

		\subsubsection{Hopkins Statistic}
			Riportiamo lo score che ci dice che il nostro dataset è ben clusterizzabile

		\subsubsection{K-means}
			Dato un insieme di punti $P \subset M$, dove $M$ è uno spazio metrico indotto da una funzione distanza $d$, l'algoritmo K-means punta a costruire una partizione $C = \{ C_1, ..., C_K \}$ con centri dei cluster $\{ c_1, ..., c_K\}$ che minimizzi la seguente funzione obiettivo.

			\begin{equation}
				\Phi_{K-means} (C) = \sum_{i=1}^K \sum_{p \in C_i \cap P} d(p, c_i) \\
			\end{equation}

			Per il nostro progetto abbiamo utilizzato l'implementazione Java \emph{KMeans} di Spark \emph{mllib}, reperibile nella \href{https://spark.apache.org/docs/2.1.0/mllib-clustering.html#k-means}{documentazione} online.

		\subsubsection{Latent Dirichlet Allocation}

			Vediamo cosa viene fuori e un bel grafico

		\subsubsection{Altri metodi}
			La libreria \emph{mllib} contiene altre tecniche, tra cui \emph{Gaussian mixture} e \emph{Bisecting k-means}, ma esse si sono rivelate inapplicabili, la prima per l'eccessivo tempo di calcolo e la seconda perché affine al già citato \emph{K-means}, ottimizzando infatti la stessa funzione obiettivo.

	\subsection{Valutazioni del Clustering}

		\subsubsection{Simplified Silhouette}
			Silhouette è un metodo unsupervised per interpretare e validare un dato cluster.
			In particolare questa tecnica calcola per ogni punto la distanza media dal proprio cluster e la minima distanza media dagli altri cluster, valutando così sia la coesione che la separazione dei cluster, e restituisce un valore compreso tra $-1$ e $1$.
			Tale approccio tuttavia richiede tempo $\mathcal{O}(n^2)$, dove $n$ è la taglia dell'input.
			Tuttavia questa complessità quadratica, considerata la taglia del nostro dataset e le risorse di calcolo a nostra disposizione, rende poco pratico l'utilizzo di tale tecnica.

			Pertanto abbiamo provveduto a implementare e applicare una versione modificata che approssima questo l'algoritmo, denominata Simplified Silhouette \cite{sscmapreduce}.
			Indicando con $a$ la distanza tra il punto considerato e il centro del proprio cluster e con $b$ la minima distanza tra il punto e gli altri centri del cluster, il Simplified Silhouette Coefficient è così definito:

			\begin{equation} \begin{aligned} \label{eq:simplifiedSilhouette}
				SSC = \frac{b - a}{ max(a, b) } = \frac{b - a}{b}
			\end{aligned} \end{equation}

			Si nota che il denominatore viene semplificato nel secondo passaggio.
			Ciò è possibile perché, nel caso ad esempio di Kmeans, la distanza euclidea tra il punto e il suo centro sarà sempre minore della distanza tra il punto e un qualsiasi altro centroide.
			Se così non fosse significherebbe che la distanza tra il punto considerato e il centroide di un altro cluster è minore che tra il punto e il centro del proprio cluster, ma questo è assurdo in quanto Kmeans forma i cluster proprio assegnando ogni punto al centroide più vicino.

			Questa approssimazione permette quindi di ottenere una buona stima di Silhouette \cite{ssc} e riduce la complessità dell'algoritmo a $\mathcal{O}(kn)$, dove $k$ è il numero di cluster e $n$ il numero di punti in input.
			Tale fattore di miglioramento delle prestazioni si indebolisce tuttavia all'aumentare di $k$.
			In particolare per $k \to n$ si ritorna a una complessità quadratica, tuttavia in generale si cerca un numero di cluster minore di $n$ e nel nostro caso permette un apprezzabile miglioramento di performance.

			I singoli score vengono poi mediati sulla taglia dell'input, generando così un'indicazione globale sulla bontà del clustering.

			Un'ulteriore osservazione è il fatto che, con l'aumentare di $k$, il coefficiente calcolato con Silhouette, e lo stesso vale per Simplified Silhouette, tende a $1$ (il miglior punteggio possibile) e raggiunge effettivamente tale valore con $k = n$ in quanto in questo caso estremo la distanza di ogni punto dal proprio centro è nulla, essendo ogni elemento un cluster a sé stante e pertanto l'unico elemento del cluster.
			Indicando con $d$ la distanza media tra il punto e il suo cluster e con $min\,d$ la minima distanza media tra il punto e gli altri cluster, si ha quindi che:

			\begin{equation} \begin{aligned} \label{eq:silhouetteToOne}
				Silhouette = \frac{min\,d - d}{ max(d, min\,d) } \quad \xrightarrow [k \to n] \quad \\ 1
			\end{aligned} \end{equation}

			Ciò può portare ad una certa difficoltà nell'utilizzo di Silhouette come strumento per individuare un $k$ ottimale, in quanto $k$ molto elevati sono portati ad ottenere punteggi molto buoni.
			Tale problematica può essere affrontata introducendo ad esempio un peso al coefficiente restituito da Silhouette in maniera da evitare questa convergenza a $1$.
			Dato che questa indagine esula dagli scopi di questo lavoro non abbiamo trattato oltre questo punto che tuttavia può essere oggetto di studi futuri.

		\subsubsection{Normalized Mutual Information} \label{sec:NMI}
			L'informazione mutua è una quantità che misura la mutua dipendenza di due variabili aleatorie, ovvero quanta informazione porta sull'altra la conoscenza del valore di una delle due.

			Questa misura può essere impiegata per valutare quanto due partizioni, o \emph{clustering}, concordano nel suddividere un set di punti \cite{Manning}.

			Per fare questo, ad ogni cluster è stata associata una variabile indicatrice $\omega$, che assume valore 1 se il punto considerato appartiene al cluster e 0 altrimenti.
			Ogni clustering viene perciò individuato dall'insieme $\Omega$ di queste variabili aleatorie mutualmente esclusive e a somma unitaria.

			Con questa descrizione del problema è possibile calcolare l'informazione mutua tra due distinti clustering $\Omega$ e $\Phi$.
			Questa matrica è stata normalizzata in $(0, 1)$ per garantire un confronto alla pari tra clustering di dimensione diversa.

			NMI viene quindi definita come
			\begin{equation} \begin{aligned} \label{eq:NMI}
				& NMI(\Omega, \Phi) = \frac
					{I(\Omega, \Phi)}
					{\left[ H(\Omega) + H(\Phi)\right] / 2} \\
				& \text{dove} \\
				& I(\Omega, \Phi) =
					\sum_{\omega \in \Omega} \sum_{\phi \in \Phi}
						P(\omega \cap \phi) \log \frac {P(\omega \cap \phi)} {P(\omega) P(\phi)} \\
				& H(\Omega) = - \sum_{\omega \in \Omega} P(\omega) \log P(\omega) \\
				& H(\Phi) = - \sum_{\phi \in \Phi} P(\phi) \log P(\phi) \\
			\end{aligned} \end{equation}

			All'atto pratico, come valore delle probabilità sono stati impegate stime a massima verosimiglianza, per esempio
			\begin{equation*}
				P(\omega) = \frac
					{ \text{numero di punti in }\omega }
					{ \text{numero di punti totali} }
			\end{equation*}.

			\bigbreak

			Purtroppo questa definizione non è direttamente applicabile al confronto tra cluster e categorie perché, mentre i clustering ottenuti con K-means e LDA sono delle effettive partizioni del dataset, non si può dire lo stesso delle categorie, dato che un articolo può possederne più di una.

			Il primo approccio per scogliere questo nodo è stato quello di eseguire un \emph{ranking} con \emph{Inverse Document Frequency} (vedi sezione \ref{sec:bag_of_words}) tra le categorie di ciascun articolo per eleggere la più rappresentativa.
			Grazie a questo passaggio NMI può essere calcolata direttamente dalla sua definizione (equazione \ref{eq:NMI}).

			Il secondo approccio tenta invece di estendere NMI al caso di cluster che si sovrappongano l'uno all'altro, considerando quindi ogni classe $c$ con la sua complementare $\bar{c}$ una partizione dell'insieme dei punti.
			L'informazione mutua viene calcolata quindi per ogni clustering $C = \{c, \bar{c}\}$ e si valuta la loro somma.

\section{Risultati}

	\subsection{Numero di cluster}
		Il clustering K-means risulta il più semplice e rapido da ottenere e per questo motivo abbiamo deciso di ispezionare un ampio range di valori per K, numero di cluster.

		La funzione obiettivo cala continuamente al variare del numero di cluster, come da figura \ref{fig:KMeansObj}, ma il calo diventa sempre più trascurabile al crescere di K.
		Questa osservazione è confermata dall'andamento della derivata della funzione obiettivo: essa raggiunge un tasso di incremento sostanzialmente nullo in prossimità del valore K=100.

		\begin{figure}[!htb]
			\hspace{-2cm}
			\begin{subfigure}{.5\textwidth}
				\centering
				\includegraphics[scale=.5]{../results/NMI-kmeans-categories.eps}
			\end{subfigure}
			\hspace{1.5cm}
			\begin{subfigure}{.5\textwidth}
				\centering
				\includegraphics[scale=.5]{../results/KMeansDerivative.eps}
			\end{subfigure}
			\caption{La funzione obiettivo smette di calare in modo significativo tra 50 e 150.}
			\label{fig:KMeansObj}
		\end{figure}

	Per questo motivo nelle analisi successive ci siamo concentrati su valori di K compresi tra 50 e 150.

	\subsection{Confronto tra i Cluster ottenuti con Normalize Mutual Information}
		Come si puo notare in figura \ref{fig:NMI-kmeans-categories}, l'andamento dell'informazione mutua del clustering rispetto alle categorie risulta analogo per entrambi gli approcci impiegati, descritti nella sezione \ref{sec:NMI}.

		L'aumento di informazione al crescre di K ci suggerisce che aumentando il numero di cluster si ottiene un clustering più vicino alla suddivisione indotta dalle categorie, ma che il miglioramento è tanto meno apprezzabile quanto più si sale con il valore di K.

		\begin{figure}[!htbp]
			\hspace{-2cm}
			\begin{subfigure}{.5\textwidth}
				\centering
				\includegraphics[scale=.5]{../results/NMI-kmeans-ranked-categories.eps}
			\end{subfigure}
			\hspace{1.5cm}
			\begin{subfigure}{.5\textwidth}
				\centering
				\includegraphics[scale=.5]{../results/NMI-kmeans-overlapping-categories.eps}
			\end{subfigure}
			\caption{NMI tra K-means e il clustering indotto dalle categorie}
			\label{fig:NMI-kmeans-categories}
		\end{figure}

	\subsection{Validazione con Simplified Silhouette}
		\begin{figure}[!htb]
			\centering
			\includegraphics[scale=.5]{../results/silhouette.eps}
			\caption{Silhouette}
			\label{fig:silhouette}
		\end{figure}

\section{Conclusioni}
	Le conclusioni generali dalle analisi effettuate
	Proposte di punti da approfondire in studi futuri

%------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%------------------------------------------------------------------------------

\begin{thebibliography}{12}

\bibitem{w2vdim}
    Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations
    in vector space. ICLR Workshop, 2013.

\bibitem{Manning}
	Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008

\bibitem{bagofwords}
	Harris, Zellig S. "Distributional structure." Word 10.2-3 (1954): 146-162.

\bibitem{lda}
	Blei, David M., Andrew Y. Ng, and Michael I. Jordan. "Latent dirichlet allocation." Journal of machine Learning research 3.Jan (2003): 993-1022.

\bibitem{sscmapreduce}
	Garcia, Kemilly Dearo, and Murilo Coelho Naldi. "Multiple parallel mapreduce k-means clustering with validation and selection." Intelligent Systems (BRACIS), 2014 Brazilian Conference on. IEEE, 2014.

\bibitem{ssc}
	Eler, Danilo Medeiros, et al. "Simplified Stress and Simplified Silhouette Coefficient to a Faster Quality Evaluation of Multidimensional Projection Techniques and Feature Spaces." Information Visualisation (iV), 2015 19th International Conference on. IEEE, 2015.

\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}
