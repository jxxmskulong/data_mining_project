%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (10/6/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
12pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Generazione delle categorie di wikipedia attraverso il clustering}} % The article title

\author{\spacedlowsmallcaps{Cazzaro Dalla Cia Lovisotto Vianello}}

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block





\newpage

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduzione}

Per il progetto del corso di Data Mining abbiamo deciso di svolgere la 
traccia proposta B, che proponeva di indagare fino a che punto un clustering sulle pagine di Wikipedia è consistente con le categorie associate alle pagine stesse.
Per fare ciò avevamo a disposizione due dataset, di differenti grandezze, formati da articoli di Wikipedia in lingua inglese.

A partire da tali premesse ci siamo prefissati i seguenti quesiti:

\begin{itemize}
	\item quanto sono clusterizzabili gli articoli del dataset?
	\item le categorie associate a questi articoli sono sensate?
	\item quali sono dei metodi validi per rappresentare opportunamente i nostri dati in input?
	\item come variano i cluster utilizzando tecniche diverse?
	\item c'è un numero ottimale di cluster per il dataset? Quanto vale?
	\item qual è il rapporto tra il cluster ottenuto e le categorie?
\end{itemize}

Tali quesiti troveranno risposta nel seguito della trattazione, che è strutturata nel modo che segue.
La sezione 2 illustra l'analisi preliminare effettuata sugli articoli e sulle categorie.
Successivamente il capitolo 3 descrive le tecniche utilizzate per rappresentare il dataset.
Il capitolo 4 tratta poi degli algoritmi di clustering applicati al dataset e delle tecniche di valutazione impiegate.
Di seguito la sezione 5 riporta e discute i risultati ottenuti nel capitolo precedente, offrendo anche un confronto tra i metodi utilizzati.
Infine la sezione 6 riassume il lavoro svolto, ciò che abbiamo ottenuto e propone alcuni spunti di ricerca per futuri sviluppi.


\section{Dataset e Analisi Preliminare}

Il dataset che abbiamo utilizzato per effettuare questa analisi non è altro che un dump di wikipedia.
Tale Dump è composto da un JSON contenente centomila articoli di wikipedia in versione inglese. Per ogni articolo di
wikipedia abbiamo a disposizione il titolo, il testo, l'id e le categorie dell'articolo.

La nostra analisi mira a valutare gli articoli e le loro categorie quindi prima di effettuare
tale analisi si è deciso di eliminare tutti gli articoli i quali non risultano essere associati a
nessuna categorie. Tale pagine sono dovute al fatto che in wikipedia sono presenti della pagine di
disambiguazione e quindi non sono utili per i nostri scopi.

//todo analisi delle categorie (sort e distribuzione delle categorie)

\section{Rappresentazione del Dataset}

	Prima di procedere con qualsiasi operazione sull'intero corpus si è deciso di preprocessare il data set
    eliminando le stop words presenti nel testo e lemmatizzando del corpus rimanente. Per l'eliminazione delle
    stop words ci siamo basati su una lista di parole fornita dal sito http://www.ranks.nl/stopwords.

	\subsection{VETTORIALIZZIONE DEGLI ARTICOLI}

		%allenamento word2vec con i suoi vari parametri (dare una motivazione per la dimensione 100)

        Per interagire con i più comuni algoritmi di clustering, ad esempio K-means, si è reso necessario trasformare
        gli articoli di Wikipedia in vettori.
        Per fare ciò abbiamo adottato una tecnica nota in letteratura con il nome Word2Vec. Tale algoritmo ideato da
        Tomas Mikolov non è altro che una rete neurale a due strati il cui scopo è quello di trasformare parole del
        linguaggio naturale in vettori. Nello spazio vettoriale generato le parole semanticamente più simili
        saranno più vicine, viceversa parole semanticamente diverse risulteranno distanti.

        Tale funzionalità è già implementata nella suite software di Spark,
        per sfruttarla è necessario inizializzare alcuni parametri di settaggio. Tra questi uno dei più importanti
        è sicuramente la dimensione del vettore in uscita. In letteratura si è valutato che una dimensionalità
        nel ordine dei 100/300 \cite{w2vdim}
        è sufficiente a rappresenta un buon compromesso in termini di performance.
        Uno volta settati i parametri l'algoritmo di Word2Vec necessita di essere allenato. Tale allenamento è stato
        fatto su tutto il corpus.

        Per trasformare un articolo è stato sufficiente effettuare la media vettoriale di tutte le parole presenti
        nel testo di un'articolo. Tale operazione è stata eseguita attraverso il BLAS (Basic Linear Algebra Subprograms)
        di Spark per eseguire tali conti nella maniera più efficiente possibile.




	\subsection{BAG OF WORDS}

		//todo: la trasformazione in bag of words



\section{Clustering}

	\subsection{Tecniche di Clustering}

		\subsubsection{Hopkins Statistic}

			Riportiamo lo score che ci dice che il nostro dataset è ben clusterizzabile

		\subsubsection{Kmeans}

			Kmeans e il suo score con un bel grafico

		\subsubsection{Altri metodi}

			Abbiamo provato anche il clustering gerarchico e il Gaussian Mixture Model
			ma non abbiamo abbastanza potenza di calcolo

		\subsubsection{Latent Dirichlet Allocation}

			Vediamo cosa viene fuori e un bel grafico


	\subsection{Valutazioni del Clustering}

		\subsubsection{Simple Silhouette}

			Utilizzo della versione semplificata di Silhouette con i centroidi
			Rimozione dei cluster con un solo articolo dal punteggio
			Magari buttiamoci un peso a sta metrica

		\subsubsection{Normalized Mutual Information}

			Il problema di individuare una funzione obiettivo che utilizzi le categorie,
			le quali non formano una partizione in quanto overlapping
			Pulizia delle categorie con con idf
			Modifica dell'algoritmo



\section{Risultati}

	\subsection{Numero di cluster}

		Il K selezionato dalle due tecniche Kmeans e LDA
		Speriamo sia simile!

	\subsection{Validazione con Simple Silhouette}

		L'andamento sempre crescente della Silhouette
		Un bel grafico lineare

	\subsection{Confronto tra i Cluster ottenuti con Normalize Mutual Information}

		Confronto tramite NMI delle due tecniche Kmeans e LDA


\section{Conclusioni}

	Le conclusioni generali dalle analisi effettuate

	Proposte di punti da approfondire in studi futuri




%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\begin{thebibliography}{12}

\bibitem{w2vdim}
    Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations
    in vector space. ICLR Workshop, 2013.


\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}
