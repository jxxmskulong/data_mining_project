package it.unipd.dei.dm1617.examples;

import it.unipd.dei.dm1617.WikiPage;
import it.unipd.dei.dm1617.WikiV;
import org.apache.hadoop.mapred.join.ArrayListBackedIterator;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaRDDLike;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.mllib.clustering.KMeans;
import org.apache.spark.mllib.clustering.KMeansModel;
import org.apache.spark.mllib.linalg.Vector;

import scala.Tuple2;

import java.util.ArrayList;
import java.io.File;
/**
 * @param path of directory where file generated by Doc2Vec are saved
 * @author massimo
 */
public class Cluster {
    public static void main(String[] args){
        String path = args[0];
        if(!path.endsWith("/")){
            path=path + "/";
        }

        // usual Spark setup
        SparkConf conf = new SparkConf(true).setAppName("Tf-Ifd transformation");
        JavaSparkContext sc = new JavaSparkContext(conf);
        sc.setLogLevel("ERROR");

        // mark the starting point of our subsequent messages
        System.out.println("###################################################" +
            "#################################################################");

        // load Doc2Vec page representation, i.e. tuples (wikipage_id, vector),
        // from the multiple output files
        System.out.println("load files");
        ArrayList<JavaRDD<Tuple2<Long, Vector>>> wikiVectors = new ArrayList();
        File folder = new File(path);
        for (File file : folder.listFiles()) {
            String fName=file.getName();
            if (file.isFile() && !fName.startsWith("_") && !fName.startsWith(".")) {
                wikiVectors.add(sc.objectFile(path + fName));
            }
        }

        // merge all chunks in  a single RDD
        System.out.println("get a unique file");
        JavaRDD<Tuple2<Long, Vector>> allWikiVector = wikiVectors.remove(0);
        for(JavaRDD<Tuple2<Long, Vector>> app:wikiVectors){
            allWikiVector = allWikiVector.union(app);
        }

        // remove id, since clustering requires RDD of Vectors
        JavaRDD<Vector> onlyVectors = allWikiVector.map(elem -> {
            return elem._2();
        });

        // cluster the data into two classes using KMeans
        System.out.println("Performing clustering");
        int numClusters = 60;
        int numIterations = 20;
        KMeansModel clusters = KMeans.train(onlyVectors.rdd(), numClusters, numIterations);

        // retrieve corresponding group for each of the input data,
        // to associate each cluster with the actual WikiPages
        JavaRDD<Integer> clusterIDs = clusters.predict(onlyVectors);

        // create an RDD with (cluster_id, (wikipage_id, vector))
        JavaPairRDD<Integer, Tuple2<Long, Vector>> completeDataset =
            clusterIDs.zip(allWikiVector);

        // map each row to a json string representation, as a general save / load
        // format
        JavaRDD<String> jsonDataset = completeDataset.map((tuple)->{
            return "{" +
                    "\"id_cluster\": " + tuple._1() + "," +
                    "\"id_wiki\": " + tuple._2()._1() + "," +
                    "\"vector\": " + tuple._2()._2()
                   + "}";
        });

        // collapse all parallel outputs to a single RDD (1) and save
        // this is needed to have a single output file
        jsonDataset.coalesce(1).saveAsTextFile("output/kmeans");
    }

}
