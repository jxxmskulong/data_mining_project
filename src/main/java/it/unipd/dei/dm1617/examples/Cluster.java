package it.unipd.dei.dm1617.examples;

import it.unipd.dei.dm1617.WikiPage;
import it.unipd.dei.dm1617.WikiV;
import org.apache.hadoop.mapred.join.ArrayListBackedIterator;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaRDDLike;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.mllib.clustering.KMeans;
import org.apache.spark.mllib.clustering.KMeansModel;
import org.apache.spark.mllib.linalg.Vector;

import scala.Tuple2;

import java.util.ArrayList;
import java.io.File;
/**
 * @param path of directory where file generated by Doc2Vec are saved
 * @author massimo
 */
public class Cluster {
    public static void main(String[] args){
        String path = args[0];
        if(!path.endsWith("/")){
            path=path+"/";
        }

        // Usual setup
        SparkConf conf = new SparkConf(true).setAppName("Tf-Ifd transformation");
        JavaSparkContext sc = new JavaSparkContext(conf);
        sc.setLogLevel("ERROR");
        System.err.println("##############################################################################################################################");

        //load file
        System.err.println("load files");
        ArrayList<JavaRDD<Tuple2<Long, Vector>>> wikiVectors = new ArrayList();
        File folder = new File(path);
        File[] listOfFiles = folder.listFiles();
        for (File file : listOfFiles) {
            String fName=file.getName();
            if (file.isFile() && !fName.startsWith("_") && !fName.startsWith(".")) {
                wikiVectors.add(sc.objectFile(path+""+fName));
                //System.err.println(path+""+fName);
            }
        }

        System.err.println("get a unique file");
        JavaRDD<Tuple2<Long, Vector>> allWikiVector = wikiVectors.remove(0);
        for(JavaRDD<Tuple2<Long, Vector>> app:wikiVectors){
            allWikiVector = allWikiVector.union(app);
        }
        //System.err.println("tot "+allWikiVector.count());


        JavaRDD<Vector> onlyVectors = allWikiVector.map(elem->{
            return elem._2();
        });

        // Cluster the data into two classes using KMeans
        System.err.println("clustering");
        int numClusters = 60;
        int numIterations = 20;
        KMeansModel clusters = KMeans.train(onlyVectors.rdd(), numClusters, numIterations);

        //get vectors cluster
        JavaRDD<Integer> indici = clusters.predict(onlyVectors);

        //join the 2 RDD in only one

        JavaPairRDD<Integer,Tuple2<Long, Vector>> all = indici.zip(allWikiVector);

        JavaRDD<String> out = all.map((tuple)->{
            String str=
                    "{"+"\"id_cluster\": "+tuple._1()+"," +
                    "\"id_wiki\": "+tuple._2()._1()+"," +
                    "\"vector\": "+tuple._2()._2();
            str+="}";
            return str;

        });
        JavaRDD<String> finalOut = out.coalesce(1);
        System.err.println("Salvo il file");
        finalOut.saveAsTextFile("output/clusterResult");
    }

}
